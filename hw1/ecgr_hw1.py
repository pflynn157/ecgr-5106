# -*- coding: utf-8 -*-
"""ECGR-HW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vGuofhfThaLACOXrJrC2Jp0MdxapMv51

## ECGR Homework 1

Patrick Flynn
801055057
"""

pip install d2l==1.0.0b0

"""# Part 1

**Problem 1:**
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torch
import time
import torchvision
from torchvision import transforms
from d2l import torch as d2l
from torch import nn
import torch.nn.functional as F

d2l.use_svg_display()

class FashionMNIST(d2l.DataModule):
    def __init__(self, batch_size=64, resize=(28,28)):
        super().__init__()
        self.save_hyperparameters()
        trans = transforms.Compose([transforms.Resize(resize), transforms.ToTensor()])
        self.train = torchvision.datasets.FashionMNIST(
            root=self.root, train=True, transform=trans, download=True
        )
        self.val = torchvision.datasets.FashionMNIST(
            root=self.root, train=False, transform=trans, download=True
        )

@d2l.add_to_class(FashionMNIST)
def text_labels(self, indicies):
    labels = [ "t-shirt", "trouser", "pullover", "dress", "coat",
              "sandal", "shirt", "sneaker", "bag", "ankele boot" ]
    return [labels[int(i)] for i in indicies]

@d2l.add_to_class(FashionMNIST)
def get_dataloader(self, train):
    data = self.train if train else self.val
    return torch.utils.data.DataLoader(data, self.batch_size, shuffle=train,
                                       num_workers=self.num_workers)
    
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):
    raise NotImplementedError

@d2l.add_to_class(FashionMNIST)
def visualize(self, batch, nrows=1, ncols=8, labels=[]):
  X, Y = batch
  if not labels:
      labels = self.text_labels(Y)
  d2l.show_images(X.squeeze(1), nrows, ncols, titles=labels)

data = FashionMNIST(resize=(32,32))
print(len(data.train))
print(len(data.val))
print(data.train[0][0].shape)

print("")

X, Y = next(iter(data.train_dataloader()))
print(X.shape, X.dtype, Y.shape, Y.dtype)

print("")

batch = next(iter(data.val_dataloader()))
data.visualize(batch)

"""Next part"""

class Classifier(d2l.Module):
    def validation_step(self, batch):
        Y_hat = self(*batch[:-1])
        self.plot("loss", self.loss(Y_hat, batch[-1]), train=False)
        self.plot("acc", self.accuracy(Y_hat, batch[-1]), train=False)

@d2l.add_to_class(d2l.Module)
def configure_optimizers(self):
    return torch.optim.SGD(self.parameters(), lr=self.lr)
  
@d2l.add_to_class(Classifier)
def accuracy(self, Y_hat, Y, averaged=True):
    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
    preds = Y_hat.argmax(axis=1).type(Y.dtype)
    compare = (preds == Y.reshape(-1)).type(torch.float32)
    return compare.mean() if averaged else compare

@d2l.add_to_class(Classifier)
def loss(self, Y_hat, Y, averaged=True):
    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
    Y = Y.reshape((-1,))
    return F.cross_entropy(Y_hat, Y, reduction="mean" if averaged else "none")

class SoftmaxRegression(d2l.Classifier):
    def __init__(self, num_outputs, hl1, hl2, hl3, lr):
        super().__init__()
        self.save_hyperparameters()
        #self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_outputs))
        #self.net = nn.Sequential(nn.Flatten(), nn.Flatten(), nn.Flatten(), nn.LazyLinear(num_outputs))
        self.net = nn.Sequential(nn.Flatten(),
                          nn.LazyLinear(hl1), nn.ReLU(),
                          nn.LazyLinear(hl2), nn.ReLU(),
                          nn.LazyLinear(hl3), nn.ReLU(),
                          nn.LazyLinear(num_outputs)
                    )
    
    def forward(self, X):
        return self.net(X)

"""### Standard Training

1.a. train the model from scratch (with randomized parameters) and plot the results (training loss and accuracy, validation accuracy) after 20 epochs. Does your network need more epochs for full training? Do you observe overfitting? Make sure to save the trained parameters and model. 
"""

import time
time1 = time.time()

data = d2l.FashionMNIST(batch_size=256)
model = SoftmaxRegression(num_outputs=10, hl1=512, hl2=256, hl3=256, lr=0.1)
trainer = d2l.Trainer(max_epochs=20)
trainer.fit(model, data)

time2 = time.time() - time1
print("Training time from scratch (20 epochs): " + str(time2))

"""Print our metrics and save them for future loading:"""

print("Our Model: \n\n", model, '\n')
print("The State Dict Keys: \n\n", model.state_dict().keys())

torch.save(model.state_dict(), 'MultiLayer_Perceptron_Model.params')

"""Training with more epochs for comparison:"""

import time
time1 = time.time()

data = d2l.FashionMNIST(batch_size=256)
model = SoftmaxRegression(num_outputs=10, hl1=512, hl2=256, hl3=256, lr=0.1)
trainer = d2l.Trainer(max_epochs=50)
trainer.fit(model, data)

time2 = time.time() - time1
print("Training time from scratch: " + str(time2))

"""Reload and run:"""

# Recovering the Stored Model (Parameters)
saved_model = SoftmaxRegression(num_outputs = 10, hl1 = 512, hl2 = 256, hl3 = 256, lr = 0.1)
saved_model.load_state_dict(torch.load('MultiLayer_Perceptron_Model.params'))
saved_model.eval()

import time
time1 = time.time()

trainer = d2l.Trainer(max_epochs = 20)
trainer.fit(saved_model, data)

time2 = time.time() - time1
print("Training time from loading: " + str(time2))

"""### Weight Decay

1.b Report section a; this time add weight penalties (weight decays). Report and plot your training results. how do the training results change compared to the baseline? (15pts)
"""

class WeightDecay(Classifier):
    def __init__(self, num_inputs, hl1, hl2, hl3, wd, lr, sigma=0.01):
        super().__init__(lr)
        self.save_hyperparameters()
        self.wd = wd
        self.weights = torch.normal(0, sigma, (hl1, 1), requires_grad=True)
        self.net = nn.Sequential(nn.Flatten(),
                                 nn.LazyLinear(hl1), nn.ReLU(),
                                 nn.LazyLinear(hl2), nn.ReLU(),
                                 nn.LazyLinear(hl3), nn.ReLU(),
                                 nn.LazyLinear(num_inputs))

    def loss(self, Y_hat, Y, averaged=True):
        Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
        Y = Y.reshape((-1))
        return (super().loss(Y_hat, Y) + self.wd * l2_penalty(self.weights))
    
    def forward(self, X):
        return self.net(X)

def l2_penalty(w):
    return (w ** 2).sum() / 2

data = d2l.FashionMNIST(batch_size=256)
trainer = d2l.Trainer(max_epochs=20)

def train_scratch():
    start_time = time.time()
    model.board.yscale='log'
    trainer.fit(model, data)
    end_time = time.time() - start_time
    print("Total time training: " + str(end_time))
    print("")

model = WeightDecay(num_inputs=10, hl1=512, hl2=256, hl3=256, wd=1, lr=0.01)
train_scratch()
torch.save(model.state_dict(), 'MLP_weightDecay.params')

"""### With Dropout

1.c Report section a; this time add dropout (dropout = 0.3). Report and plot your training results. how do the training results change compared to the baseline? How the training results change compared to the weight penalties. 
"""

class DropOut(Classifier):
    def __init__(self, num_inputs, hl1, hl2, hl3, dpo1, dpo2, dpo3, lr, sigma=0.01):
        super().__init__(lr)
        self.save_hyperparameters()
        self.net = nn.Sequential(nn.Flatten(),
                                 nn.LazyLinear(hl1), nn.ReLU(), nn.Dropout(dpo1),
                                 nn.LazyLinear(hl2), nn.ReLU(), nn.Dropout(dpo2),
                                 nn.LazyLinear(hl3), nn.ReLU(), nn.Dropout(dpo3),
                                 nn.LazyLinear(num_inputs))

    def loss(self, Y_hat, Y, averaged=True):
        Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
        Y = Y.reshape((-1))
        return F.cross_entropy(Y_hat, Y, reduction = "mean" if averaged else "none")
    
    def forward(self, X):
        return self.net(X)

def l2_penalty(w):
    return (w ** 2).sum() / 2

data = d2l.FashionMNIST(batch_size=256)
trainer = d2l.Trainer(max_epochs=20)
model = DropOut(num_inputs=10, hl1=512, hl2=256, hl3=256, dpo1=0.3, dpo2=0.3, dpo3=0.3, lr=0.01)
start_time = time.time()
trainer.fit(model, data)
end_time = time.time() - start_time
print("Total time training: " + str(end_time))
print("")

"""# Part 2

Kaggle Dataset

Part one: setup and load everything
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import pandas as pd
import torch
import time
import torchvision
from torchvision import transforms
from d2l import torch as d2l
from torch import nn
import torch.nn.functional as F

# Create the class for our data
class KaggleHouse(d2l.DataModule):
    def __init__(self, batch_size, train=None, val=None):
        super().__init__()
        self.save_hyperparameters()
        if self.train is None:
            self.raw_train = pd.read_csv(d2l.download(
                d2l.DATA_URL + 'kaggle_house_pred_train.csv', self.root,
                sha1_hash='585e9cc93e70b39160e7921475f9bcd7d31219ce'))
            self.raw_val = pd.read_csv(d2l.download(
                d2l.DATA_URL + 'kaggle_house_pred_test.csv', self.root,
                sha1_hash='fa19780a7b011d9b009e8bff8e99922a8ee2eb90'))
    
    def preprocess(self):
        # Remove the ID and label columns
        features = pd.concat((
            self.raw_train.drop(columns=["Id", "SalePrice"]),
            self.raw_val.drop(columns=["Id"])
        ))

        # Standardize the numerical columns
        num_features = features.dtypes[features.dtypes != "object"].index
        features[num_features] = features[num_features].apply(lambda x: (x-x.mean()) / x.std())

        # Replace any NANs with 0
        features[num_features] = features[num_features].fillna(0)

        # Replace with discrete features by one-hot encoding
        features = pd.get_dummies(features, dummy_na=True)

        # Finally, save all features
        self.train = features[:self.raw_train.shape[0]].copy()
        self.train["SalePrice"] = self.raw_train["SalePrice"]
        self.val = features[self.raw_train.shape[0]:].copy()

# Load the data set
data = KaggleHouse(batch_size = 64)
print(data.raw_train.shape)
print(data.raw_val.shape)

print("-------")

data.preprocess()
print(data.train.shape)
print(data.val.shape)

data.raw_train

# Visualize our preprocessed data
data.train

@d2l.add_to_class(KaggleHouse)
def get_dataloader(self, train):
    label = "SalePrice"
    data = self.train if train else self.val
    if label not in data: return
    get_tensor = lambda x: torch.tensor(x.values, dtype=torch.float32)
    tensors = (get_tensor(data.drop(columns=[label])),
               torch.log(get_tensor(data[label])).reshape((-1, 1)))
    return self.get_tensorloader(tensors, train)

"""Our training loop"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torch
from d2l import torch as d2l

class LinearRegression(d2l.Module):
    def __init__(self, lr):
        super().__init__()
        self.save_hyperparameters()
        #self.net = nn.LazyLinear()
        #self.net = nn.ReLU()
        #self.net = nn.LazyLinear()
        self.net = nn.Sequential(
                                 nn.LazyLinear(256), nn.ReLU(), #nn.Dropout(),
                                 nn.LazyLinear(256), nn.ReLU(), #nn.Dropout(),
                                 nn.LazyLinear(256), nn.ReLU(), #nn.Dropout(),
                                 #nn.LazyLinear(512)
                                 )
        #self.net.weight.data.normal_(0, 0.01)
        #self.net.bias.data.fill_(0)

@d2l.add_to_class(LinearRegression)
def forward(self, X):
    """The linear regression model."""
    return self.net(X)

@d2l.add_to_class(LinearRegression)
def loss(self, y_hat, y):
    fn = nn.MSELoss()
    return fn(y_hat, y)

@d2l.add_to_class(LinearRegression)
def configure_optimizers(self):
    return torch.optim.SGD(self.parameters(), self.lr)

"""Training:"""

@d2l.add_to_class(d2l.Trainer)
def prepare_batch(self, batch):
    return batch

@d2l.add_to_class(d2l.Trainer)
def fit_epoch(self):
    self.model.train()
    for batch in self.train_dataloader:
        loss = self.model.training_step(self.prepare_batch(batch))
        self.optim.zero_grad()
        with torch.no_grad():
            loss.backward()
            if self.gradient_clip_val > 0:  # To be discussed later
                self.clip_gradients(self.gradient_clip_val, self.model)
            self.optim.step()
        self.train_batch_idx += 1
    if self.val_dataloader is None:
        return
    self.model.eval()
    for batch in self.val_dataloader:
        with torch.no_grad():
            self.model.validation_step(self.prepare_batch(batch))
        self.val_batch_idx += 1

print(data.train.shape)
print(data.val.shape)

"""KFold"""

def k_fold_data(data, k):
    rets = []
    fold_size = data.train.shape[0] // k
    for j in range(k):
        idx = range(j * fold_size, (j+1) * fold_size)
        rets.append(KaggleHouse(data.batch_size, data.train.drop(index=idx),
                                data.train.loc[idx]))
    return rets

def k_fold(trainer, data, k, lr):
    val_loss, models = [], []
    for i, data_fold in enumerate(k_fold_data(data, k)):
        model = LinearRegression(lr)
        model.board.yscale='log'
        if i != 0: model.board.display = False
        trainer.fit(model, data_fold)
        val_loss.append(float(model.board.data['val_loss'][-1].y))
        models.append(model)
    print(f'average validation log mse = {sum(val_loss)/len(val_loss)}')
    return models

"""2.a What happens if we need to standardize the continuous numerical features like what we have done in this section?"""

trainer = d2l.Trainer(max_epochs=20)
models = k_fold(trainer, data, k=5, lr=0.01)

"""2.b Improve the score by improving the model complexity. Please plot the training results and compare them against the baseline model we did in the lectures. How about the model complexity comparison and training time? (15pt)"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import torch
from d2l import torch as d2l

class LinearRegression(d2l.Module):
    def __init__(self, lr):
        super().__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(
                                 nn.LazyLinear(512), nn.ReLU(), #nn.Dropout(),
                                 nn.LazyLinear(512), nn.ReLU(), #nn.Dropout(),
                                 nn.LazyLinear(512), nn.ReLU(), #nn.Dropout(),
                                 nn.LazyLinear(512), nn.ReLU(), #nn.Dropout(),
                                 nn.LazyLinear(512), nn.ReLU(), #nn.Dropout(),
                                 nn.LazyLinear(512), nn.ReLU(), #nn.Dropout(),
                                 )
        #self.net.weight.data.normal_(0, 0.01)
        #self.net.bias.data.fill_(0)

@d2l.add_to_class(LinearRegression)
def forward(self, X):
    """The linear regression model."""
    return self.net(X)

@d2l.add_to_class(LinearRegression)
def loss(self, y_hat, y):
    fn = nn.MSELoss()
    return fn(y_hat, y)

@d2l.add_to_class(LinearRegression)
def configure_optimizers(self):
    return torch.optim.SGD(self.parameters(), self.lr)

trainer = d2l.Trainer(max_epochs=20)
models = k_fold(trainer, data, k=5, lr=0.01)

"""Make a submission of predictions:"""

preds = [model(torch.tensor(data.val.values, dtype=torch.float32))
         for model in models]
# Taking exponentiation of predictions in the logarithm scale
ensemble_preds = torch.exp(torch.cat(preds, 1)).mean(1)
submission = pd.DataFrame({'Id':data.raw_val.Id,
                           'SalePrice':ensemble_preds.detach().numpy()})
submission.to_csv('submission.csv', index=False)